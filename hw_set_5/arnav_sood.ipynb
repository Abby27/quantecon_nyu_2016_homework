{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5 for ECON-GA 3002\n",
    "\n",
    "**{Name, ID, Email}:** {Arnav Sood, N11193569, asood@nyu.edu}\n",
    "\n",
    "**Overview**: Show the downward bias in the standard OLS estimate of the lag coefficient by running a large number of trials for a few (alpha, n) pairs, and then plotting. Use Julia. Optimize for speed.\n",
    "\n",
    "**Note on Speed**: When I checked this before, it looked like code ran about twice as slowly on the Ubuntu VM as on OS X. There's also a phenomenon where I observe greater memory allocation, and quicker speed, in serial on the VM than in parallel. Notes should be factored in when considering the speed of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define parameters. We use the `const` keyword wherever possible, because that allows the compiler to optimize (otherwise, it would have to permit any range of types and values). We also use the `global` keyword -- the downside is a global variable, but the upside is a pre-allocated 10k array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "const alphas = collect(0.5:0.1:0.9);\n",
    "const ns = collect(50:50:500);\n",
    "const beta = 1;\n",
    "global biases = zeros(Float64,10000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define a function to calculate the bias for one run, given parameters. Note the use of a `vals` reference in the argument signature -- this allows us to sidestep declaring a new array of zeros each time we evaluate one run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bias(alpha, n, beta,vals)\n",
    "    vals[1] = randn()*100;\n",
    "     for i in 2:n\n",
    "        vals[i] = vals[i-1]*alpha + randn()  + beta;\n",
    "     end\n",
    "     xs = vals[1:n-1];\n",
    "     ys = vals[2:n];\n",
    "     sumx = sum(xs);\n",
    "     sumy = sum(ys);\n",
    "     sumxsq = round(Int,norm(xs,2)^2);\n",
    "     sumxy = dot(xs,ys);\n",
    "     val = (sumxy-(n-1)*(sumx/(n-1))*(sumy/(n-1)))/(sumxsq-(n-1)*((sumx/(n-1))^2)) - alpha;\n",
    "     # For Type Stability -- Apparently the compiler likes this.\n",
    "     return typeof(val) == Float64 ? val : 0.0;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the ternary operator we use in the return statement. The reason for this construction is that the Julia compiler optimizes, in large part, based on type -- a type-stable function allows the compiler more certainty about the function's behavior, which translates into quicker code.\n",
    "\n",
    "> Now, we define a function `aggregatebias`, which runs and aggregates the biases from 10,000 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatebias (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function aggregatebias(alpha, n, beta)\n",
    "     vals = zeros(n);\n",
    "     for i in 1:10000\n",
    "        biases[i] = bias(alpha, n, beta,vals);\n",
    "     end\n",
    "     avg = mean(biases);\n",
    "     return avg;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, initialize and declare a `results` vector, to hold the biases across each of the 50 parameter tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = zeros(Float64,50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a `main` function, which will evaluate and compile the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "   for i in 1:50\n",
    "        results[i] =\n",
    "        aggregatebias(alphas[round(Int,ceil(i/10))],ns[round(Int,ceil(i/5))],beta);\n",
    "    end\n",
    "    print(results)\n",
    "    return results\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.004539565973038508,-0.0046865799074916215,-0.004366197099032988,-0.00468809043635621,-0.004250835423886926,-0.0033909003337137863,-0.0027969991614667416,-0.0022664471588514537,-0.002389420597869988,-0.0029123067082936055,-0.002478034698421887,-0.0027895196155986036,-0.0029707752825583944,-0.002234149283055882,-0.0028195422274161996,-0.0019975455305236574,-0.0021483171340686406,-0.0020494721299255296,-0.002366144756182333,-0.0020244696569242937,-0.0022577984874553514,-0.002012056666747344,-0.002006783360241693,-0.00251986967560708,-0.002107942624269211,-0.0021032607829150765,-0.002142027748736898,-0.002267330384641003,-0.00214932425491374,-0.0019697630134741244,-0.0019745475086671268,-0.002256311351313352,-0.0021298788456212887,-0.002022302709758398,-0.0019423070703335412,-0.0018330623496171448,-0.0020261068068975915,-0.0019270282931204853,-0.0017249174379351821,-0.0018186366982917619,-0.002039788652178863,-0.001947320651550936,-0.001919650175332911,-0.0018124499089069918,-0.0018373320261705042,-0.001799846063243929,-0.0016705613733509093,-0.0017453749068250808,-0.0019621419449273514,-0.0016541096650425171]  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "50-element Array{Float64,1}:\n",
       " -0.00453957\n",
       " -0.00468658\n",
       " -0.0043662 \n",
       " -0.00468809\n",
       " -0.00425084\n",
       " -0.0033909 \n",
       " -0.002797  \n",
       " -0.00226645\n",
       " -0.00238942\n",
       " -0.00291231\n",
       " -0.00247803\n",
       " -0.00278952\n",
       " -0.00297078\n",
       "  â‹®         \n",
       " -0.00172492\n",
       " -0.00181864\n",
       " -0.00203979\n",
       " -0.00194732\n",
       " -0.00191965\n",
       " -0.00181245\n",
       " -0.00183733\n",
       " -0.00179985\n",
       " -0.00167056\n",
       " -0.00174537\n",
       " -0.00196214\n",
       " -0.00165411"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.766692 seconds (2.26 M allocations: 2.177 GB, 8.94% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot. [Handle this later]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We See\n",
    "\n",
    "Recall what we observed in the previous assignment:\n",
    "\n",
    "> Consistency: There is a clear asymptotic trend to 0 bias. This is because the OLS estimator is a consistent one, which means that it has exactly this property.\n",
    "\n",
    "> Downward Bias: All the data points are negative, which confirms our supposition that the OLS estimator is downward-biased.\n",
    "\n",
    "> Proportionality: This isn't a formal statistical term, but roughly speaking, we see that the magnitude of the bias increases with the absolute value of alpha.\n",
    "\n",
    "> Convergence Rate: We see a logarithmic-type convergence rate, which implies diminishing returns to additional data points, and a clear steep portion to avoid when running simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
