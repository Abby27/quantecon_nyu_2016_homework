{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 4\n",
    "Felipe Alves - N14713445\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question \n",
    "Show the downward bias of the OLS estimate of $\\alpha$ in the AR1 process\n",
    "\n",
    "$$ X_{t+1} = \\beta + \\alpha X_{t} + \\sigma W_{t+1}, \\quad \\{W_t\\} \\sim \\mathcal{N}(0,1) $$\n",
    "\n",
    "for sample sizes $n=50,100,150, \\ldots, 500$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import quantecon as qe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import randn\n",
    "from numpy.linalg import inv\n",
    "from numba import jit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def ar1_sim(bet,alp,sig,n):\n",
    "    \"\"\"\n",
    "    Simulates an AR(1) of form \n",
    "    \n",
    "    .. math::\n",
    "\n",
    "    X_{t+1} = β + α X_t + σ W_{t+1}\n",
    "    \n",
    "    and return { X }_{t=0}^{T} and { X }_{t=1}^{T+1}\n",
    "    for regression\n",
    "    \"\"\"\n",
    "    eps = randn(n)\n",
    "    x = np.zeros(n+1)\n",
    "    x[0] = bet/(1-alp)\n",
    "    for i in range(n):\n",
    "        x[i+1] = bet + alp*x[i] + eps[i]\n",
    "\n",
    "    return x[0:-1],x[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested two alternative \n",
    "\n",
    "  * My jit ar1_sim function \n",
    "  \n",
    "  * ARMA class in the Quan Econ packages\n",
    "  \n",
    "testing how much time it takes to simulate the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.16 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 42.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "timeit y = ar1_sim(0.0,0.5,1.0,int(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lp = qe.ARMA(0.5)\n",
    "\n",
    "%time data = lp.simulation(int(10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, since the exercise deal with a specific AR(1) process we don't need \n",
    "the generality offered by the ARMA class and can work with \n",
    "the simple ar1_sim function, that has the benefit of being way faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alp_hat(alp,n,N):\n",
    "    alp_hat = 0.0\n",
    "    for k in range(N):                \n",
    "        x,y = ar1_sim(0.0,alp, 1.0, n)\n",
    "        alp0 = ols(x,y,n)\n",
    "        alp_hat += alp0[1]\n",
    "\n",
    "    alp_hat = alp_hat/N    \n",
    "    return alp_hat\n",
    "\n",
    "alp_hat_numba = jit(alp_hat)  \n",
    "\n",
    "def ols(x,y,n):\n",
    "    b0 = np.ones(n)\n",
    "    X = np.concatenate((b0, x))\n",
    "    XT = X.reshape(2,n)\n",
    "    X = XT.T\n",
    "    alp_hat = np.dot(inv(np.dot(XT,X)),(np.dot(XT,y)))\n",
    "    \n",
    "    return alp_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 528 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit α = alp_hat(0.5,100,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1.32 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit α = alp_hat_numba(0.5,100,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(Eα̂, A, nsize, N=1000, print_plot = True):\n",
    "    \"\"\"\n",
    "    Compute the bias for different levels of \n",
    "    parameter α and sample size n\n",
    "    \n",
    "    \"\"\"\n",
    "    for j,alp in enumerate(A):\n",
    "        i=0\n",
    "        for n in nsize:\n",
    "            Eα̂[j,i] = alp_hat(alp,n,N)\n",
    "            i+=1\n",
    "\n",
    "    if print_plot:\n",
    "        fig,ax = plt.subplots(figsize =(8,5))\n",
    "        for i,α in enumerate(A):\n",
    "            current_label = r'$\\alpha = {0:.1f}$'.format(α)\n",
    "            ax.plot(list(nsize),Eα̂[i,:]-α, 'o', alpha = 0.7, label=current_label)\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.set_xlabel(r'Sample Size')\n",
    "        ax.set_ylabel(r'Bias')\n",
    "        plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# α values\n",
    "A =  (0.5,0.6,0.7,0.8,0.9)\n",
    "# Sample size\n",
    "nsize = range(50,500,50)\n",
    "\n",
    "# OLS estimates\n",
    "Eα̂ = np.zeros((len(A),len(nsize)))\n",
    "\n",
    "%time main(Eα̂,A,nsize,10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
