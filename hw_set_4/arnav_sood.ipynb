{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 for ECON-GA 3002, Section 12.\n",
    "\n",
    "**{Name, N-Number, Email}**: {Arnav Sood, N11193569, asood@nyu.edu}\n",
    "\n",
    "**Overview**: Show the downward bias in the standard OLS estimate of the lag coefficient by running a large number of trials for a few (alpha, n) pairs, and then plotting. Optimize for speed.\n",
    "\n",
    "**Disclosure**: Was not written using vim (sorry). Was instead written using a Jupyter notebook and REPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vanilla Python version. Before optimizing, get a vanilla code set in Python.\n",
    "\n",
    "# Update: parallelization is hard.\n",
    "\n",
    "# The optimization path will probably look something like this:\n",
    "'''\n",
    "    - Write vanilla Python code.\n",
    "            - Wall Time: 2m41s\n",
    "    - Make easy Python improvements (e.g, @jit, numbafy)\n",
    "            - @jit: Wall Time: 17.9s\n",
    "            - @jit on Mac (no VM): 11.3s\n",
    "            - add numpy: 2s?\n",
    "            - remove print: like 5s?\n",
    "    - Make harder Python improvements (not sure what they are a priori)\n",
    "    - Write code in C if it makes sense\n",
    "    - Figure out what PEP8 is, and make sure I'm using it\n",
    "'''\n",
    "\n",
    "# Throw in numba jit compile.\n",
    "from numba import jit\n",
    "\n",
    "# Add more numpy stuff.\n",
    "import numpy as np\n",
    "\n",
    "# Setup variables.\n",
    "alphas = np.arange(0.5,1,0.1)\n",
    "ns = np.arange(50,550,50)\n",
    "\n",
    "# Define a function to take an (alpha, n, beta) tuple, and return the bias for the estimate.\n",
    "\n",
    "# Store this as a lambda function so it can compile the operation once. (?)\n",
    "estimate = lambda sumx, sumy, sumxsq, sumxy, n : (sumxy - (n-1)*(sumx/(n-1))*(sumy/(n-1)))/(sumxsq - (n-1)*math.pow((sumx/(n-1)),2));\n",
    "\n",
    "@jit\n",
    "def bias(alpha, n, beta):\n",
    "        sumx = 0\n",
    "        sumy = 0\n",
    "        sumxy = 0\n",
    "        sumxsq = 0\n",
    "        x = rand()*100\n",
    "        for i in range(n):\n",
    "            sumx += x\n",
    "            sumxsq += math.pow(x,2)\n",
    "            y = beta + alpha*x + rand()\n",
    "            sumxy += x*y\n",
    "            sumy += y\n",
    "            x = y\n",
    "        val = estimate(sumx, sumy, sumxsq, sumxy, n) - alpha\n",
    "        return val\n",
    "\n",
    "# Define a function to take an (alpha, n, beta) tuple, estimate 10,000 times, and then average to get the expected bias.\n",
    "from numpy.random import normal as rand\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def calcbias(alpha, n, beta):\n",
    "    biases = np.zeros(10000)\n",
    "    for i in range(10000):\n",
    "        biases[i] = bias(alpha,n,beta)\n",
    "    avg = np.mean(biases)\n",
    "    return avg\n",
    "\n",
    "# Define a function to call the above on the right sets of params, and return the biases.\n",
    "@jit\n",
    "def main():\n",
    "    results = np.zeros(50)\n",
    "    for i in range(5):\n",
    "        for j in range(10):\n",
    "            results[10*i+j] = calcbias(alphas[i],ns[j],1)\n",
    "    print(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00773579 -0.00451932 -0.00405684 -0.0032614  -0.00247437 -0.00290046\n",
      " -0.00247151 -0.00230902 -0.00192862 -0.00184639 -0.00955591 -0.00605299\n",
      " -0.00431926 -0.00329249 -0.00320002 -0.00301772 -0.00254632 -0.00254722\n",
      " -0.00242114 -0.00199767 -0.01012679 -0.00598315 -0.00470899 -0.00380652\n",
      " -0.00334048 -0.00328217 -0.00292817 -0.00276288 -0.00245581 -0.00245079\n",
      " -0.0134482  -0.00737448 -0.0051959  -0.00462799 -0.00407733 -0.00350783\n",
      " -0.00310341 -0.00270505 -0.00256782 -0.00253409 -0.03771731 -0.01072232\n",
      " -0.00666914 -0.00580565 -0.00465095 -0.00411568 -0.00345366 -0.00309649\n",
      " -0.00295633 -0.00281806]\n",
      "CPU times: user 11.3 s, sys: 24.8 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%time x = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot.\n",
    "from matplotlib import pyplot as plt\n",
    "vec1 = x[0:10]\n",
    "vec2 = x[10:20]\n",
    "vec3 = x[20:30]\n",
    "vec4 = x[30:40]\n",
    "vec5 = x[40:50]\n",
    "\n",
    "# Plot each of the vectors against 'n'\n",
    "plt.plot(ns, vec1, 'r-', ns, vec2, 'b-', ns, vec3, 'o-',ns,vec4,'g-',ns,vec5,'k-')\n",
    "plt.title('Biases vs. Sample Sizes in OLS Estimate of Lag Coefficient')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Bias (Absolute Difference)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What We See\n",
    "\n",
    "There are a few properties of the OLS estimator which we can extract from the figure:\n",
    "\n",
    "**Consistency**: There is a clear asymptotic trend to 0 bias. This is because the OLS estimator is a consistent one, which means that it has exactly this property.\n",
    "\n",
    "**Downward "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
