{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 for ECON-GA 3002, Section 12.\n",
    "\n",
    "**{Name, N-Number, Email}**: {Arnav Sood, N11193569, asood@nyu.edu}\n",
    "\n",
    "**Overview**: Show the downward bias in the standard OLS estimate of the lag coefficient by running a large number of trials for a few (alpha, n) pairs, and then plotting. Optimize for speed.\n",
    "\n",
    "**Disclosure**: Was not written using vim (sorry). Was instead written using a Jupyter notebook and REPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vanilla Python version. Before optimizing, get a vanilla code set in Python.\n",
    "\n",
    "# Update: parallelization is hard.\n",
    "\n",
    "# The optimization path will probably look something like this:\n",
    "'''\n",
    "    - Write vanilla Python code.\n",
    "            - Wall Time: 2m41s\n",
    "    - Make easy Python improvements (e.g, @jit, numbafy)\n",
    "            - @jit: Wall Time: 17.9s\n",
    "            - @jit on Mac (no VM): 11.3s\n",
    "            - add numpy: 2s?\n",
    "            - remove print: like 5s?\n",
    "    - Make harder Python improvements (not sure what they are a priori)\n",
    "    - Write code in C if it makes sense\n",
    "    - Figure out what PEP8 is, and make sure I'm using it\n",
    "'''\n",
    "\n",
    "# Throw in numba jit compile.\n",
    "from numba import jit\n",
    "\n",
    "# Add more numpy stuff.\n",
    "import numpy as np\n",
    "\n",
    "# Setup variables.\n",
    "alphas = np.arange(0.5,1,0.1)\n",
    "ns = np.arange(50,550,50)\n",
    "\n",
    "# Define a function to take an (alpha, n, beta) tuple, and return the bias for the estimate.\n",
    "\n",
    "# Store this as a lambda function so it can compile the operation once. (?)\n",
    "estimate = lambda sumx, sumy, sumxsq, sumxy, n : (sumxy - (n-1)*(sumx/(n-1))*(sumy/(n-1)))/(sumxsq - (n-1)*math.pow((sumx/(n-1)),2));\n",
    "\n",
    "@jit\n",
    "def bias(alpha, n, beta):\n",
    "        sumx = 0\n",
    "        sumy = 0\n",
    "        sumxy = 0\n",
    "        sumxsq = 0\n",
    "        x = rand()*100\n",
    "        for i in range(n):\n",
    "            sumx += x\n",
    "            sumxsq += math.pow(x,2)\n",
    "            y = beta + alpha*x + rand()\n",
    "            sumxy += x*y\n",
    "            sumy += y\n",
    "            x = y\n",
    "        val = estimate(sumx, sumy, sumxsq, sumxy, n) - alpha\n",
    "        return val\n",
    "\n",
    "# Define a function to take an (alpha, n, beta) tuple, estimate 10,000 times, and then average to get the expected bias.\n",
    "from numpy.random import normal as rand\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def calcbias(alpha, n, beta):\n",
    "    biases = np.zeros(10000)\n",
    "    for i in range(10000):\n",
    "        biases[i] = bias(alpha,n,beta)\n",
    "    avg = np.mean(biases)\n",
    "    return avg\n",
    "\n",
    "# Define a function to call the above on the right sets of params, and return the biases.\n",
    "@jit\n",
    "def main():\n",
    "    results = np.zeros(50)\n",
    "    for i in range(5):\n",
    "        for j in range(10):\n",
    "            results[10*i+j] = calcbias(alphas[i],ns[j],1)\n",
    "    print(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0068893  -0.00530987 -0.00393713 -0.00302123 -0.00277637 -0.00274908\n",
      " -0.00222198 -0.00203061 -0.00190814 -0.0020535  -0.00898535 -0.00548835\n",
      " -0.00399966 -0.00385827 -0.00325968 -0.00292754 -0.00269609 -0.00234095\n",
      " -0.00239163 -0.00188504 -0.01009127 -0.00591049 -0.00478598 -0.00390847\n",
      " -0.00383308 -0.00319646 -0.00268235 -0.00278073 -0.00258312 -0.00237767\n",
      " -0.01509515 -0.00741615 -0.00544057 -0.00418215 -0.00353113 -0.00343919\n",
      " -0.00324128 -0.00300572 -0.00257797 -0.00265793 -0.01577805 -0.01135966\n",
      " -0.00680381 -0.00508683 -0.00447353 -0.0040401  -0.00371721 -0.00329455\n",
      " -0.0028915  -0.00283044]\n",
      "CPU times: user 11.9 s, sys: 38.1 ms, total: 11.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%time x = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot.\n",
    "from matplotlib import pyplot as plt\n",
    "vec1 = x[0:10]\n",
    "vec2 = x[10:20]\n",
    "vec3 = x[20:30]\n",
    "vec4 = x[30:40]\n",
    "vec5 = x[40:50]\n",
    "\n",
    "# Plot each of the vectors against 'n'\n",
    "plt.plot(ns, vec1, 'r-', ns, vec2, 'b-', ns, vec3, 'm-',ns,vec4,'g-',ns,vec5,'k-')\n",
    "plt.title('Biases vs. Sample Sizes in OLS Estimate of Lag Coefficient')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Bias (Absolute Difference)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What We See\n",
    "\n",
    "There are a few properties of the OLS estimator which we can extract from the figure:\n",
    "\n",
    "**Consistency**: There is a clear asymptotic trend to 0 bias. This is because the OLS estimator is a consistent one, which means that it has exactly this property.\n",
    "\n",
    "**Downward Bias**: All the data points are negative, which confirms our supposition that the OLS estimator is downward-biased.\n",
    "\n",
    "**Proportionality**: This isn't a formal statistical term, but roughly speaking, we see that the magnitude of the bias increases with the absolute value of alpha.\n",
    "\n",
    "**Convergence Rate**: We see a logarithmic-type convergence rate, which implies diminishing returns to additional data points, and a clear steep portion to avoid when running simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
