{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 for ECON-GA 3002, Section 12.\n",
    "\n",
    "**{Name, N-Number, Email}**: {Arnav Sood, N11193569, asood@nyu.edu}\n",
    "\n",
    "**Overview**: Show the downward bias in the standard OLS estimate of the lag coefficient by running a large number of trials for a few (alpha, n) pairs, and then plotting. Optimize for speed.\n",
    "\n",
    "**Disclosure**: Was not written using vim. Was instead written using a Jupyter notebook and REPL,across Linux and OS X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vanilla Python version. Before optimizing, get a vanilla code set in Python.\n",
    "\n",
    "**Update: Parallelization is Hard**\n",
    "\n",
    "**The optimization path will probably look something like this:**\n",
    "    - Write vanilla Python code.\n",
    "            - Wall Time: 2m41s\n",
    "    - Make easy Python improvements (e.g, @jit, numbafy)\n",
    "            - @jit: Wall Time: 17.9s\n",
    "            - @jit on Mac (no VM): 11.3s\n",
    "            - add numpy: 2s?\n",
    "            - remove print: like 5s?\n",
    "    - Make harder Python improvements (not sure what they are a priori)\n",
    "    - Write code in C if it makes sense\n",
    "    - Figure out what PEP8 is, and make sure I'm using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perform some setup tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Throw in numba jit compile.\n",
    "from numba import jit\n",
    "\n",
    "# Add more numpy stuff.\n",
    "import numpy as np\n",
    "\n",
    "# Setup variables.\n",
    "alphas = np.arange(0.5,1,0.1)\n",
    "ns = np.arange(50,550,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write a function to take an (alpha, n, beta) tuple, and return the bias for the estimate.\n",
    "> Write also a function to take all the values, and compute an estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimate = lambda sumx, sumy, sumxsq, sumxy, n : (sumxy - (n-1)*(sumx/(n-1))*(sumy/(n-1)))/(sumxsq - (n-1)*math.pow((sumx/(n-1)),2));\n",
    "\n",
    "@jit\n",
    "def bias(alpha, n, beta):\n",
    "        sumx = 0\n",
    "        sumy = 0\n",
    "        sumxy = 0\n",
    "        sumxsq = 0\n",
    "        x = rand()*100\n",
    "        for i in range(n):\n",
    "            sumx += x\n",
    "            sumxsq += math.pow(x,2)\n",
    "            y = beta + alpha*x + rand()\n",
    "            sumxy += x*y\n",
    "            sumy += y\n",
    "            x = y\n",
    "        val = estimate(sumx, sumy, sumxsq, sumxy, n) - alpha\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define a function to take an (alpha, n, beta) tuple, estimate 10,000 times, and then average to get the expected bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import normal as rand\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def calcbias(alpha, n, beta):\n",
    "    biases = np.zeros(10000)\n",
    "    for i in range(10000):\n",
    "        biases[i] = bias(alpha,n,beta)\n",
    "    avg = np.mean(biases)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define a function to call the above on the right sets of params, and return the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def main():\n",
    "    results = np.zeros(50)\n",
    "    for i in range(5):\n",
    "        for j in range(10):\n",
    "            results[10*i+j] = calcbias(alphas[i],ns[j],1)\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, run computations and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00771547 -0.0051092  -0.00365801 -0.00336739 -0.00271695 -0.00273799\n",
      " -0.00229746 -0.00203893 -0.00197101 -0.00208153 -0.00833809 -0.0063318\n",
      " -0.00429925 -0.00379574 -0.00292781 -0.00269557 -0.00303331 -0.00249969\n",
      " -0.00243595 -0.00207987 -0.01097336 -0.00656129 -0.00487487 -0.00382249\n",
      " -0.0036508  -0.00309616 -0.00287937 -0.00243803 -0.00266576 -0.00210318\n",
      " -0.01388954 -0.00739954 -0.0057068  -0.00443808 -0.00399948 -0.00365017\n",
      " -0.00321158 -0.00330154 -0.00242881 -0.00257117 -0.05318519 -0.01072521\n",
      " -0.00709341 -0.00525823 -0.00452473 -0.00399    -0.00377148 -0.00331809\n",
      " -0.00297456 -0.00294658]\n",
      "CPU times: user 11 s, sys: 13.9 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%time x = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split the data by alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "vec1 = x[0:10]\n",
    "vec2 = x[10:20]\n",
    "vec3 = x[20:30]\n",
    "vec4 = x[30:40]\n",
    "vec5 = x[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot each of the vectors against 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ns, vec1, 'r-', ns, vec2, 'b-', ns, vec3, 'm-',ns,vec4,'g-',ns,vec5,'k-')\n",
    "plt.title('Biases vs. Sample Sizes in OLS Estimate of Lag Coefficient')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Bias (Absolute Difference)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What We See\n",
    "\n",
    "There are a few properties of the OLS estimator which we can extract from the figure:\n",
    "\n",
    "**Consistency**: There is a clear asymptotic trend to 0 bias. This is because the OLS estimator is a consistent one, which means that it has exactly this property.\n",
    "\n",
    "**Downward Bias**: All the data points are negative, which confirms our supposition that the OLS estimator is downward-biased.\n",
    "\n",
    "**Proportionality**: This isn't a formal statistical term, but roughly speaking, we see that the magnitude of the bias increases with the absolute value of alpha.\n",
    "\n",
    "**Convergence Rate**: We see a logarithmic-type convergence rate, which implies diminishing returns to additional data points, and a clear steep portion to avoid when running simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
